{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensor2tensor.models import transformer\n",
    "from tensor2tensor.layers import common_layers\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttentionCNN(transformer.Transformer):\n",
    "    \"\"\"Sequence-CNN with interwoven self-attention.\"\"\"\n",
    "    # Gated Transcription Factor cOnvolutions (GTFO)\n",
    "    \n",
    "    def bottom(self, features):\n",
    "        # ids ==> DNA embeddings\n",
    "        # assume there are 5 IDs: ACTGN\n",
    "        features[\"inputs\"] = common_layers.embedding(\n",
    "            features[\"inputs\"],\n",
    "            vocab_size=5,\n",
    "            dense_size=self._hparams.hidden_size)\n",
    "        return features\n",
    "    \n",
    "    def top(self, body_output, unused_features):\n",
    "        # body_output ==> logits [batch_size, target_length]\n",
    "        return tf.reduce_mean(body_output, axis=-1)\n",
    "        \n",
    "    def loss(self, logits, features):\n",
    "        # logits ==> loss\n",
    "        labels = features[\"targets\"]\n",
    "        loss_num = tf.losses.sigmoid_cross_entropy(\n",
    "            labels, logits, label_smoothing=hparams.label_smoothing)\n",
    "        loss_denom = 1.0\n",
    "        return (loss_num, loss_denom)\n",
    "    \n",
    "    def body(self, features):\n",
    "        hparams = self._hparams\n",
    "        \n",
    "        inputs = features[\"inputs\"]\n",
    "        target_space = features[\"target_space_id\"]\n",
    "\n",
    "        # force these settings\n",
    "        full_att_conv = (\"dot_product\", \"conv_relu_conv\")\n",
    "        local_att_fc = (\"local_unmasked\", \"dense_relu_dense\")\n",
    "        \n",
    "        # by default conv ffn uses a kernel of size 3\n",
    "        # by default local att uses a kernel size of 128\n",
    "        hparams.ffn_layer, hparams.self_attention_type = full_att_conv\n",
    "        body_output, _ = self.encode(inputs, target_space, hparams, features=features)\n",
    "        \n",
    "        targets = features[\"targets\"]\n",
    "        target_length = targets.shape[1]\n",
    "        \n",
    "        # dense connection along input_length\n",
    "        layer_input = tf.transpose(body_output, [0, 2, 1]) # [batch_size, hidden_dim, input_length]\n",
    "        layer_output = common_layers.dense(layer_input, target_length, activation=tf.nn.relu)\n",
    "        \n",
    "        body_output = common_layers.layer_postprocess(layer_input, layer_output, hparams)\n",
    "        body_output = tf.transpose(body_output, [0, 2, 1])  # [batch_size, target_length, hidden_dim]\n",
    "\n",
    "        return body_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
