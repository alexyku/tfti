{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/akmorrow/anaconda2/envs/python36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /data/akmorrow/anaconda2/envs/python36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import collections\n",
    "\n",
    "from tensor2tensor import models\n",
    "from tensor2tensor import problems\n",
    "from tensor2tensor.layers import common_layers\n",
    "from tensor2tensor.utils import trainer_lib\n",
    "from tensor2tensor.utils import t2t_model\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor.utils import metrics\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USER CONFIGURES ###\n",
    "data_dir = \"/data/akmorrow/tfti/t2t_data\"\n",
    "train_dir = \"/data/akmorrow/tfti/t2t_train\"\n",
    "validation_file = \"/data/akmorrow/tfti/data/deepseavalidation.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsetting shared_embedding_and_softmax_weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-23 13:28:48,678] Unsetting shared_embedding_and_softmax_weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting T2TModel mode to 'eval'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-23 13:28:48,679] Setting T2TModel mode to 'eval'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting hparams.dropout to 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-23 13:28:48,681] Setting hparams.dropout to 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting hparams.layer_prepostprocess_dropout to 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-23 13:28:48,682] Setting hparams.layer_prepostprocess_dropout to 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting hparams.symbol_dropout to 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-23 13:28:48,684] Setting hparams.symbol_dropout to 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting hparams.attention_dropout to 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-23 13:28:48,685] Setting hparams.attention_dropout to 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting hparams.relu_dropout to 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-23 13:28:48,686] Setting hparams.relu_dropout to 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:TFs for CellType H1-hESC: ['SP1', 'Pol2-4H8', 'USF2', 'NRSF', 'RFX5', 'c-Myc', 'RXRA', 'EZH2', 'TBP', 'CHD1', 'Egr-1', 'SIN3A', 'GABP', 'CEBPB', 'Nrf1', 'p300', 'CTCF', 'ATF3', 'ATF2', 'Pol2', 'BCL11A', 'BRCA1', 'TCF12', 'SIX5', 'JunD', 'Rad21', 'YY1', 'USF-1', 'Max', 'TAF1', 'CHD2', 'Mxi1', 'SRF', 'Znf143']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-23 13:28:48,713] TFs for CellType H1-hESC: ['SP1', 'Pol2-4H8', 'USF2', 'NRSF', 'RFX5', 'c-Myc', 'RXRA', 'EZH2', 'TBP', 'CHD1', 'Egr-1', 'SIN3A', 'GABP', 'CEBPB', 'Nrf1', 'p300', 'CTCF', 'ATF3', 'ATF2', 'Pol2', 'BCL11A', 'BRCA1', 'TCF12', 'SIX5', 'JunD', 'Rad21', 'YY1', 'USF-1', 'Max', 'TAF1', 'CHD2', 'Mxi1', 'SRF', 'Znf143']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using variable initializer: uniform_unit_scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-23 13:28:48,729] Using variable initializer: uniform_unit_scaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Transforming feature 'inputs' with symbol_modality_782_8.bottom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-23 13:28:48,760] Transforming feature 'inputs' with symbol_modality_782_8.bottom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Transforming feature 'latents' with binary_imputation_class_label_modality_8.bottom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-23 13:28:48,872] Transforming feature 'latents' with binary_imputation_class_label_modality_8.bottom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Transforming 'targets' with binary_class_label_modality_8.targets_bottom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-23 13:28:48,891] Transforming 'targets' with binary_class_label_modality_8.targets_bottom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Building model body\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-23 13:28:48,909] Building model body\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Transforming body output with binary_class_label_modality_8.top\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-23 13:28:50,091] Transforming body output with binary_class_label_modality_8.top\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"../tfti\")\n",
    "import tfti\n",
    "\n",
    "# Reset graph for conistency.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Prepare model.\n",
    "problem_name = \"genomics_binding_deepsea_gm12878\"\n",
    "model_name = \"tfti_transformer\"\n",
    "hparams_set = \"tfti_transformer_debug\"\n",
    "hparams_overrides_str = \"\"\n",
    "\n",
    "data_dir = os.path.expanduser(data_dir)\n",
    "#output_dir = os.path.expanduser(train_dir + f\"/{problem_name}/{model_name}-{hparams_set}\")\n",
    "output_dir = os.path.expanduser(\"/data/akmorrow/tfti/t2t_train/genomics_binding_deepsea_gm12878/tfti_transformer-params_tfti_transformer_debug\")\n",
    "\n",
    "# Prepare model.\n",
    "hparams = trainer_lib.create_hparams(hparams_set, hparams_overrides_str, data_dir, problem_name)\n",
    "problem = registry.problem(problem_name)\n",
    "encoders = problem.get_feature_encoders(data_dir)\n",
    "\n",
    "# Prepare the model and the graph when model runs on features.\n",
    "model = registry.model(model_name)(hparams, tf.estimator.ModeKeys.EVAL)\n",
    "\n",
    "# Prepare features for feeding into the model.\n",
    "inputs_ph = tf.placeholder(dtype=tf.string, shape=[ ])\n",
    "targets_ph = tf.placeholder(dtype=tf.int64, shape=[problem.num_binary_predictions])\n",
    "features = {\"inputs\": inputs_ph, \"targets\": targets_ph}\n",
    "features = problem.preprocess_dev_example(features, tf.estimator.ModeKeys.EVAL, hparams)\n",
    "\n",
    "# Introduce a dummy batch dimension.\n",
    "for key in features.keys():\n",
    "    features[key] = tf.expand_dims(features[key], 0)\n",
    "\n",
    "logits, losses = model(features)\n",
    "predictions = tf.nn.sigmoid(logits)\n",
    "labels = features[\"targets\"]\n",
    "\n",
    "# Evaluation metrics we want to use.\n",
    "set_auroc, _ = tfti.set_auroc(logits, labels, features)\n",
    "set_auprc, _ = tfti.set_auprc(logits, labels, features)\n",
    "average_auroc, _ = tfti.average_auroc(logits, labels, features)\n",
    "average_auprc, _ = tfti.average_auprc(logits, labels, features)\n",
    "\n",
    "# saver = tf.train.Saver()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Initialize AUC running average stuff.\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "sess.run(init_op)\n",
    "\n",
    "# Load weights from checkpoint.\n",
    "ckpts = tf.train.get_checkpoint_state(output_dir)\n",
    "ckpt = ckpts.model_checkpoint_path\n",
    "# saver.restore(sess, ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in test data\n",
    "import pandas as pd\n",
    "validation_data=pd.read_csv(validation_file, sep='\\t',header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int64'>\n",
      "(919,)\n"
     ]
    }
   ],
   "source": [
    "dummy_inputs = \"\".join(np.random.choice(list(\"NACTG\"), problem.input_sequence_length))\n",
    "dummy_targets = np.random.choice(2, [problem.num_binary_predictions])\n",
    "\n",
    "print(type(dummy_targets[0]))\n",
    "print(dummy_targets.shape)\n",
    "\n",
    "# Run on each line.\n",
    "average_auroc_numpy = sess.run(average_auroc, feed_dict={\n",
    "    inputs_ph: dummy_inputs,\n",
    "    targets_ph: dummy_targets\n",
    "})\n",
    "\n",
    "# # Run on each line.\n",
    "# set_auroc_numpy = sess.run(set_auroc, feed_dict={\n",
    "#     inputs_ph: inputs,\n",
    "#     targets_ph: targets\n",
    "# })\n",
    "\n",
    "# print(f\"AUROC is: {average_auroc_numpy}\")\n",
    "\n",
    "# print(f\"AUROC is: {set_auroc_numpy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_and_labels = []\n",
    "\n",
    "for i in range(len(validation_data)):\n",
    "    inputs = validation_data[0][i]\n",
    "    targets = np.array(list(map(int, validation_data[1][i].split(','))))\n",
    "    \n",
    "    fetch = (predictions, labels)\n",
    "    fetch_numpy = sess.run(fetch, feed_dict={\n",
    "        inputs_ph: inputs,\n",
    "        targets_ph: targets\n",
    "    })\n",
    "    \n",
    "    predictions_and_labels.append(fetch_numpy)\n",
    "    \n",
    "predictions_and_labels = [(x.squeeze(), y.squeeze())\n",
    "                          for (x, y) in predictions_and_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_numpy = np.array(predictions_and_labels)[:, 0, :]\n",
    "labels_numpy = np.array(predictions_and_labels)[:, 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfs = sorted(['SP1', 'Pol2-4H8', 'USF2', 'NRSF', 'RFX5', 'c-Myc', 'RXRA', 'EZH2', 'TBP', 'CHD1', 'Egr-1', 'SIN3A', 'GABP', 'CEBPB', 'Nrf1', 'p300', 'CTCF', 'ATF3', 'ATF2', 'Pol2', 'BCL11A', 'BRCA1', 'TCF12', 'SIX5', 'JunD', 'Rad21', 'YY1', 'USF-1', 'Max', 'TAF1', 'CHD2', 'Mxi1', 'SRF', 'Znf143'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(labels_numpy[:,0], predictions_numpy[:,0], pos_label=2)\n",
    "metrics.auc(fpr, tpr)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for i in range(len(tfs)):\n",
    "    tf = tfs[i]\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr, tpr, _ = roc_curve(labels_numpy[:,0], predictions_numpy[:,0])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    print(roc_auc)\n",
    "\n",
    "    lw = 2\n",
    "    plt.plot(fpr[2], tpr[2], color='darkorange',\n",
    "             lw=lw, label='%s (%0.2f)' % (tf, 2))\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.        , 0.33333333, 1.        ]), array([nan, nan, nan]))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr, tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(tfs)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotprod =lambda K, L:reduce(lambda z1, z2: z1+z2, map(lambda x: reduce(lambda x1, x2: x1*x2, x), zip(K, L)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reduce' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-149-c775d9c3cc7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdotprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-148-5caccbac37f4>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(K, L)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdotprod\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mz1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mz1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mz2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'reduce' is not defined"
     ]
    }
   ],
   "source": [
    "dotprod([1, 2, 3, 4], [5, 6, 7, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
