{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsetting shared_embedding_and_softmax_weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-20 20:24:50,280] Unsetting shared_embedding_and_softmax_weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting T2TModel mode to 'eval'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-20 20:24:50,284] Setting T2TModel mode to 'eval'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting hparams.dropout to 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-20 20:24:50,287] Setting hparams.dropout to 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting hparams.layer_prepostprocess_dropout to 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-20 20:24:50,289] Setting hparams.layer_prepostprocess_dropout to 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting hparams.symbol_dropout to 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-20 20:24:50,291] Setting hparams.symbol_dropout to 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting hparams.attention_dropout to 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-20 20:24:50,294] Setting hparams.attention_dropout to 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting hparams.relu_dropout to 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-20 20:24:50,297] Setting hparams.relu_dropout to 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using variable initializer: uniform_unit_scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-20 20:24:50,356] Using variable initializer: uniform_unit_scaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Transforming feature 'inputs' with symbol_modality_782_8.bottom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-20 20:24:50,418] Transforming feature 'inputs' with symbol_modality_782_8.bottom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Transforming feature 'latents' with binary_imputation_class_label_modality_8.bottom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-20 20:24:50,632] Transforming feature 'latents' with binary_imputation_class_label_modality_8.bottom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Transforming 'targets' with binary_class_label_modality_8.targets_bottom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-20 20:24:50,669] Transforming 'targets' with binary_class_label_modality_8.targets_bottom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Building model body\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-20 20:24:50,721] Building model body\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Transforming body output with binary_class_label_modality_8.top\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-20 20:24:52,779] Transforming body output with binary_class_label_modality_8.top\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/alexyku/t2t_train/genomics_binding_deepsea_gm12878/tfti_transformer-tfti_transformer_debug-/model.ckpt-1001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-20 20:24:58,778] Restoring parameters from /Users/alexyku/t2t_train/genomics_binding_deepsea_gm12878/tfti_transformer-tfti_transformer_debug-/model.ckpt-1001\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import collections\n",
    "\n",
    "from tensor2tensor import models\n",
    "from tensor2tensor import problems\n",
    "from tensor2tensor.layers import common_layers\n",
    "from tensor2tensor.utils import trainer_lib\n",
    "from tensor2tensor.utils import t2t_model\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor.utils import metrics\n",
    "\n",
    "sys.path.append(\"../tfti\")\n",
    "import tfti\n",
    "\n",
    "# Reset graph for conistency.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Prepare model.\n",
    "problem_name = \"genomics_binding_deepsea_gm12878\"\n",
    "model_name = \"tfti_transformer\"\n",
    "hparams_set = \"tfti_transformer_debug\"\n",
    "hparams_overrides_str = \"\"\n",
    "\n",
    "data_dir = os.path.expanduser(\"~/t2t/data\")\n",
    "output_dir = os.path.expanduser(f\"~/t2t_train/{problem_name}/{model_name}-{hparams_set}-{hparams_overrides_str}\")\n",
    "\n",
    "# Prepare model.\n",
    "hparams = trainer_lib.create_hparams(hparams_set, hparams_overrides_str, data_dir, problem_name)\n",
    "problem = registry.problem(problem_name)\n",
    "encoders = problem.get_feature_encoders(data_dir)\n",
    "\n",
    "# Prepare the model and the graph when model runs on features.\n",
    "model = registry.model(model_name)(hparams, tf.estimator.ModeKeys.EVAL)\n",
    "\n",
    "# Prepare features for feeding into the model.\n",
    "inputs_ph = tf.placeholder(dtype=tf.string, shape=[])\n",
    "targets_ph = tf.placeholder(dtype=tf.int64, shape=[problem.num_binary_predictions])\n",
    "features = {\"inputs\": inputs_ph, \"targets\": targets_ph}\n",
    "features = problem.preprocess_example(features, tf.estimator.ModeKeys.EVAL, hparams)\n",
    "\n",
    "# Introduce a dummy batch dimension.\n",
    "for key in features.keys():\n",
    "    features[key] = tf.expand_dims(features[key], 0)\n",
    "\n",
    "logits, losses = model(features)\n",
    "labels = features[\"targets\"]\n",
    "\n",
    "# Evaluation metrics we want to use.\n",
    "set_auroc, _ = tfti.set_auroc(logits, labels, features)\n",
    "set_auprc, _ = tfti.set_auprc(logits, labels, features)\n",
    "average_auroc, _ = tfti.average_auroc(logits, labels, features)\n",
    "average_auprc, _ = tfti.average_auprc(logits, labels, features)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Initialize AUC running average stuff.\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "sess.run(init_op)\n",
    "\n",
    "# Load weights from checkpoint.\n",
    "ckpts = tf.train.get_checkpoint_state(output_dir)\n",
    "ckpt = ckpts.model_checkpoint_path\n",
    "saver.restore(sess, ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAGNAGCTTTTCTTNANTTNACNNAGGNAATCNNCAGTNGCAGAANACACGAGGTGACNGNGCCGGCNAGNCCGAATTNCNTTTCNCANGNNNCNANANGANCGANTCNGAGACGTTTCNAAACAGTANNTNNTCTAANCAGTNNCCCCTGTNCGCGNNCATCNCGNGCNAGGCTTNTTNCCGNTTGGCNTGGCTCTTNGNTNNAAGTTCCCTNNCATTGAANTNTGCCNATNACCGNNNTNACNNTGNTCNCGCTNAAGTTNCACTAAANANAACANTNTGGTTGGNTAATCCAACAGAAACCNCGACTCNCATNNCNCGCATTGNTTTNGGAAAGANGNACNNCTNTACGANGGGACCTCTAATTTGACGTCCCCNATNCACTCCCTCTNGNATGGATCGAANACNGCCATAANGANCTCGTAAGGNANGAGATGGCGAGAGAANNCTGTGCNNNGNTNTGATCNTTATCCGAAAGGGACTGTTCATNACNCNATAAAGTNTCNNGGGCGTACNGGTCNTNAGCCATTNGTAGTAGGNGTTGNCGCCTTCTGNNTNANACTAGGCGATNGCGTNCTNACGAGAGAGNTGTNNCNANCNGNTGNCAAACANNTNCCCANNGANTCTCTAGGCCTTCTNGCTCNNNGGTGACGCTTANTATAGCATANCNGTGNCACCCTCTTAAACAGANGNNAAGACGGCTACTANNGACCTTTGTTNNTAAGACAGCANTCNCTNGGGATATTACGTTCCACATCGACTGATNCCGACCAACAAAACGATTTNNACAGGGCNCAGCTCACNNNTGGNNCNGANNAGGNTNGGTGCTTGAATTGNGNCAACCANTGCAAAGCNGCNCCNAAGCCNANANCGGNATGNTCANTNANNGGNTTNGANTTCTCGNGNTGGNCNGAANGNACNNCGCNCCTCCTGGNTNTGACCTAGCAGTTGCCAAATAGNCATCTTTTNTTCNGNAAGCNTANTCNACCTATCGNTATTC\n",
      "[1 1 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0\n",
      " 0 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1\n",
      " 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0\n",
      " 0 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 1\n",
      " 1 1 0 0 1 1 1 0 0 0 0 1 0 1 0 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1\n",
      " 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 1 1 1 0\n",
      " 1 1 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1\n",
      " 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0 0\n",
      " 0 1 1 0 0 1 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 0\n",
      " 0 1 1 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0\n",
      " 0 1 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1\n",
      " 0 1 0 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1\n",
      " 1 0 1 1 1 0 1 0 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1\n",
      " 0 0 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 0 1 0 0 1\n",
      " 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 1 0\n",
      " 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0\n",
      " 1 0 1 1 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1\n",
      " 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 0 0 1 0 1 0 0\n",
      " 1 1 1 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0 1 1 1 0 1 0 1 1\n",
      " 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1\n",
      " 1 1 1 0 1 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1\n",
      " 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1\n",
      " 1 0 1 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1\n",
      " 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 0 1 0 0 1 1]\n",
      "AUROC is: 0.5001515746116638\n"
     ]
    }
   ],
   "source": [
    "dummy_inputs = \"\".join(np.random.choice(list(\"NACTG\"), problem.input_sequence_length))\n",
    "dummy_targets = np.random.choice(2, [problem.num_binary_predictions])\n",
    "\n",
    "print(dummy_inputs)\n",
    "print(dummy_targets)\n",
    "\n",
    "# Run on each line.\n",
    "average_auroc_numpy = sess.run(average_auroc, feed_dict={\n",
    "    inputs_ph: dummy_inputs,\n",
    "    targets_ph: dummy_targets\n",
    "})\n",
    "\n",
    "print(f\"AUROC is: {average_auroc_numpy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
